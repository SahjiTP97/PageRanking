{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc49033e-e2b9-40d9-9152-6af3c0a5af5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "from numpy.linalg import norm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab89b44a-dbd4-4853-994a-94467d9489e2",
   "metadata": {},
   "source": [
    "#### Data Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "83ff0e2e-48b1-433e-97b4-7c1e49e344dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_data(file_path):\n",
    "    # Lists to keep track of origin and destination pages\n",
    "    origin_pages = []\n",
    "    destination_pages = []\n",
    "    # Dictionaries to store out-degree and in-degree information\n",
    "    out_degree = {}\n",
    "    in_degree = {}\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            # Parse the origin and destination pages, adjusting index\n",
    "            from_page = int(line.split()[0]) - 1\n",
    "            to_page = int(line.split()[1]) - 1\n",
    "\n",
    "            origin_pages.append(from_page)\n",
    "            destination_pages.append(to_page)\n",
    "\n",
    "            # Update out-degree count for origin pages\n",
    "            if from_page not in out_degree:\n",
    "                out_degree[from_page] = 1\n",
    "            else:\n",
    "                out_degree[from_page] += 1\n",
    "\n",
    "            # Update in-degree list for destination pages\n",
    "            if to_page not in in_degree:\n",
    "                in_degree[to_page] = [from_page]\n",
    "            else:\n",
    "                in_degree[to_page].append(from_page)\n",
    "            \n",
    "    # Calculate number of edges and nodes\n",
    "    total_edges = len(origin_pages)\n",
    "    total_nodes = len(set(origin_pages) | set(destination_pages))\n",
    "\n",
    "    for node in range(total_nodes):\n",
    "        if node not in in_degree:\n",
    "            in_degree[node] = []\n",
    "\n",
    "    # Initialize sparse matrix for PageRank calculation\n",
    "    D_matrix = sparse.lil_matrix((total_nodes, 1))\n",
    "    for key in out_degree:\n",
    "        D_matrix[key] = 1 / out_degree[key]\n",
    "\n",
    "    # Construct the adjacency matrix using sparse representation\n",
    "    adjacency_matrix = sparse.csr_matrix(\n",
    "        ([1] * total_edges, (destination_pages, origin_pages)),\n",
    "        shape=(total_nodes, total_nodes)\n",
    "    )\n",
    "\n",
    "    return adjacency_matrix, D_matrix, in_degree\n",
    "\n",
    "adj_matrix, D_matrix, incoming_links_dict = parse_data(\"stanweb.dat\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eb5f90-b36e-492e-820d-5372899d24f7",
   "metadata": {},
   "source": [
    "#### 1a) Power method (a=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c11d8c7e-7970-46ce-b2e6-259da778646e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagerank (Power Method) elapsed time: 2.489877462387085 seconds\n",
      "Number of iterations: 72\n"
     ]
    }
   ],
   "source": [
    "def compute_pagerank(G, alpha=0.85, tol=1e-8):\n",
    "\n",
    "    # Number of nodes (assuming G is a square matrix)\n",
    "    num_nodes = G.shape[0]\n",
    "    \n",
    "    # Compute out-degree ratios scaled by alpha\n",
    "    out_degree_ratios = G.sum(axis=0).T / alpha\n",
    "    \n",
    "    # Initialize rank vector\n",
    "    rank_vector = np.ones((num_nodes, 1)) / num_nodes\n",
    "    error = float('inf')\n",
    "    iterations = 0\n",
    "\n",
    "    while error > tol:\n",
    "        iterations += 1\n",
    "        \n",
    "        # Compute new rank values\n",
    "        new_rank_vector = G.dot(rank_vector / out_degree_ratios)\n",
    "        \n",
    "        # Calculate leakage due to dead-ends\n",
    "        leakage = new_rank_vector.sum()\n",
    "        \n",
    "        # Reinsert leaked rank values\n",
    "        new_rank_vector += (1 - leakage) / num_nodes\n",
    "        \n",
    "        # Compute error using the Euclidean norm\n",
    "        error = np.linalg.norm(rank_vector - new_rank_vector)\n",
    "        \n",
    "        # Check for nodes that converged by the second iteration\n",
    "        if iterations == 2:\n",
    "            converged_nodes = [\n",
    "                i for i in range(rank_vector.shape[0])\n",
    "                if np.linalg.norm(rank_vector[i] - new_rank_vector[i]) < tol\n",
    "            ]\n",
    "        \n",
    "        # Update rank vector\n",
    "        rank_vector = new_rank_vector\n",
    "    \n",
    "    return rank_vector, iterations, converged_nodes\n",
    "\n",
    "import time\n",
    "\n",
    "# Measure the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute the PageRank algorithm\n",
    "page_ranks, iterations, converged_nodes = compute_pagerank(adj_matrix)\n",
    "\n",
    "# Measure and print the elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Pagerank (Power Method) elapsed time: {elapsed_time} seconds\")\n",
    "\n",
    "# Print the number of iterations\n",
    "print(\"Number of iterations:\", iterations)\n",
    "\n",
    "# Convert the PageRank vector to a flat array\n",
    "flattened_pr_vector = np.asarray(page_ranks).ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09a8c45b-79cd-4204-82f8-0cc6dc3d6285",
   "metadata": {},
   "source": [
    "#### 1a) Gauss Seidel method (a=0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8cfe1c09-ac86-44e0-b0d7-e59bc1bb2d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagerank (Gauss Seidel) elapsed time: 70.83803915977478 seconds\n",
      "Number of iterations: 56\n"
     ]
    }
   ],
   "source": [
    "def compute_pagerank_gs(tol, G, D, incoming_links, alpha):\n",
    "\n",
    "    # Initialize the rank vector with a uniform distribution\n",
    "    rank_old = np.ones(G.shape[0]) / G.shape[0]\n",
    "    iteration = 0\n",
    "    error = 1\n",
    "    uniform_prob = 1 / G.shape[0]\n",
    "    D *= alpha\n",
    "    rank = rank_old.copy()\n",
    "\n",
    "    # Gauss-Seidel method for PageRank calculation\n",
    "    while error > tol:\n",
    "        iteration += 1\n",
    "        for i in range(G.shape[0]):\n",
    "            sum_before = 0\n",
    "            sum_after = 0\n",
    "            for j in incoming_links[i]:\n",
    "                if j < i:\n",
    "                    sum_before += D.data[j][0] * rank[j]\n",
    "                if j > i:\n",
    "                    sum_after += D.data[j][0] * rank_old[j]\n",
    "\n",
    "            rank[i] = uniform_prob + sum_before + sum_after\n",
    "\n",
    "        # Calculate relative error\n",
    "        error = np.linalg.norm(rank - rank_old) / np.linalg.norm(rank)\n",
    "        if iteration == 2:\n",
    "            # Identify nodes that converged by the second iteration\n",
    "            converged_nodes = [\n",
    "                i for i in range(rank.shape[0])\n",
    "                if np.linalg.norm(rank[i] - rank_old[i]) < tol\n",
    "            ]\n",
    "\n",
    "        #print(f\"Iteration: {iteration} ==> Relative Error: {error}\")\n",
    "        rank_old = rank.copy()\n",
    "\n",
    "    return rank, iteration, converged_nodes\n",
    "\n",
    "# Measure the start time\n",
    "start_time = time.time()\n",
    "\n",
    "# Execute the PageRank algorithm using Gauss-Seidel method\n",
    "gs_ranks, iterations, converged_nodes_gs = compute_pagerank_gs(1e-8, adj_matrix, D_matrix, incoming_links_dict, 0.85)\n",
    "\n",
    "# Measure and print the elapsed time\n",
    "elapsed_time_gs = time.time() - start_time\n",
    "print(f\"Pagerank (Gauss Seidel) elapsed time: {elapsed_time_gs} seconds\")\n",
    "\n",
    "# Print the number of iterations\n",
    "print(\"Number of iterations:\", iterations)\n",
    "\n",
    "# Convert the Gauss-Seidel PageRank vector to a flat array\n",
    "flattened_gs_vector = np.asarray(gs_ranks).ravel()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32e01a86-fd5e-4a9e-89db-ef7fd4d16c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power method and Gauss Seidel method difference (a=0.85): 121652 ranks.\n"
     ]
    }
   ],
   "source": [
    "# Sort the PageRank vectors and get the indices in descending order\n",
    "sorted_indices_pr = flattened_pr_vector.argsort()[-len(flattened_pr_vector):][::-1]\n",
    "sorted_indices_prgs = flattened_gs_vector.argsort()[-len(flattened_gs_vector):][::-1]\n",
    "\n",
    "# Print the number of differing ranks between the two methods\n",
    "difference_count = np.sum(sorted_indices_pr != sorted_indices_prgs)\n",
    "print(f\"Power method and Gauss Seidel method difference (a=0.85): {difference_count} ranks.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ccd3b9-ec2f-441c-8f53-f45f9c0f77e4",
   "metadata": {},
   "source": [
    "#### 1b) Power/Gauss Seidel methods (a=0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5980e1a6-dd68-4588-b764-beffb81453ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagerank (Power Method) elapsed time: 14.6840181350708 seconds\n",
      "Number of iterations: 1141\n",
      "\n",
      "Pagerank (Gauss Seidel) elapsed time: 66.76844573020935 seconds\n",
      "Number of iterations: 53\n"
     ]
    }
   ],
   "source": [
    "# Measure the start time\n",
    "start_time = time.time()\n",
    "# Execute the PageRank algorithm with alpha=0.99 using Google's algorithm\n",
    "pr_99, iterations_99, converged_nodes_99 = compute_pagerank(adj_matrix, alpha=0.99, tol=1e-8)\n",
    "# Measure and print the elapsed time\n",
    "elapsed_time_99 = time.time() - start_time\n",
    "print(f\"Pagerank (Power Method) elapsed time: {elapsed_time_99} seconds\")\n",
    "print(\"Number of iterations:\", iterations_99)\n",
    "# Convert the PageRank vector to a flat array\n",
    "flattened_pr_vector_99 = np.asarray(pr_99).ravel()\n",
    "\n",
    "# Measure the start time\n",
    "start_time = time.time()\n",
    "# Execute the PageRank algorithm with alpha=0.99 using the Gauss-Seidel method\n",
    "gs_ranks_99, iterations_99, converged_nodes_gs_99 = compute_pagerank_gs(1e-8, adj_matrix, D_matrix, incoming_links_dict, 0.99)\n",
    "# Measure and print the elapsed time\n",
    "elapsed_time_gs_99 = time.time() - start_time\n",
    "print(f\"\\nPagerank (Gauss Seidel) elapsed time: {elapsed_time_gs_99} seconds\")\n",
    "print(\"Number of iterations:\", iterations_99)\n",
    "# Convert the Gauss-Seidel PageRank vector to a flat array\n",
    "flattened_gs_vector_99 = np.asarray(gs_ranks_99).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7290f61a-05b1-4726-9e37-00f5c43c426d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power method and Gauss Seidel method difference (a=0.99) 281887 ranks.\n",
      "49 different ranks (power method).\n",
      "33 different ranks (Gauss-Seidel).\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sort the PageRank vectors for alpha=0.99 and get the indices in descending order\n",
    "sorted_indices_pr_99 = flattened_pr_vector_99.argsort()[-len(flattened_pr_vector):][::-1]\n",
    "sorted_indices_prgs_99 = flattened_gs_vector_99.argsort()[-len(flattened_pr_vector):][::-1]\n",
    "\n",
    "# Print the number of differing ranks between the two methods for alpha=0.99\n",
    "difference_count_99 = np.sum(sorted_indices_prgs_99 != sorted_indices_pr_99)\n",
    "print(f\"Power method and Gauss Seidel method difference (a=0.99) {difference_count_99} ranks.\")\n",
    "\n",
    "# Print the number of differences in the top 50 ranks for the power method\n",
    "top_50_difference_power_method = np.sum(sorted_indices_pr[:50] != sorted_indices_pr_99[:50])\n",
    "print(f\"{top_50_difference_power_method} different ranks (power method).\")\n",
    "\n",
    "# Print the number of differences in the top 50 ranks for the Gauss-Seidel method\n",
    "top_50_difference_gauss_seidel = np.sum(sorted_indices_prgs[:50] != sorted_indices_prgs_99[:50])\n",
    "print(f\"{top_50_difference_gauss_seidel} different ranks (Gauss-Seidel).\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e568519-7e02-420f-9ff3-78e619c1a06e",
   "metadata": {},
   "source": [
    "#### 1c) Convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "125b5b6a-2d1e-4e77-a9c2-549e341edb86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Power method: 42904 components of π converge at the second iteration.\n",
      "Power method: 0 components correspond to the top 1000 ranked nodes.\n",
      "\n",
      "Gauss Seidel method: 34367 components of π converge at the second iteration.\n",
      "Gauss Seidel method: 1 components correspond to the top 1000 ranked nodes.\n"
     ]
    }
   ],
   "source": [
    "# Print the number of components that converged at the second iteration using the power method\n",
    "print(f\"Power method: {len(converged_nodes)} components of π converge at the second iteration.\")\n",
    "# Count how many of the converged components are in the top 1000 ranked nodes\n",
    "fast_converged_count_power = sum(1 for node in converged_nodes if node in sorted_indices_pr[:1000])\n",
    "print(f\"Power method: {fast_converged_count_power} components correspond to the top 1000 ranked nodes.\")\n",
    "# Print the number of components that converged at the second iteration using the Gauss-Seidel method\n",
    "print(f\"\\nGauss Seidel method: {len(converged_nodes_gs)} components of π converge at the second iteration.\")\n",
    "# Count how many of the converged components are in the top 1000 ranked nodes\n",
    "fast_converged_count_gauss_seidel = sum(1 for node in converged_nodes_gs if node in sorted_indices_prgs[:1000])\n",
    "print(f\"Gauss Seidel method: {fast_converged_count_gauss_seidel} components correspond to the top 1000 ranked nodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e47ced6-281d-44ff-b1de-e58e8025933a",
   "metadata": {},
   "source": [
    "#### 2a) Page X (no in-links, out-links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4114ddc-f455-4072-bf1e-9c16bde01350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagerank of page X: 5.333658781475117e-07\n",
      "Change in Pagerank of the other pages: 1.2048086354849138e-08\n",
      "Changes in: 50570 ranks.\n"
     ]
    }
   ],
   "source": [
    "def parse_connectivity_data(file_path):\n",
    "\n",
    "    # Lists to store the origin and destination pages\n",
    "    origin_pages = []\n",
    "    destination_pages = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read and parse each line of the data file\n",
    "        for line in file.readlines():\n",
    "            # Adjust indices to be zero-based\n",
    "            origin_page = int(line.split()[0]) - 1\n",
    "            destination_page = int(line.split()[1]) - 1\n",
    "\n",
    "            # Append pages to respective lists\n",
    "            origin_pages.append(origin_page)\n",
    "            destination_pages.append(destination_page)\n",
    "            \n",
    "    # Calculate number of edges and nodes\n",
    "    total_edges = len(origin_pages)\n",
    "    total_nodes = len(set(origin_pages) | set(destination_pages))\n",
    "    \n",
    "    # Create the sparse adjacency matrix with an additional dangling node\n",
    "    adjacency_matrix = sparse.csr_matrix(\n",
    "        ([1] * total_edges, (destination_pages, origin_pages)),\n",
    "        shape=(total_nodes + 1, total_nodes + 1)\n",
    "    )\n",
    "\n",
    "    return adjacency_matrix\n",
    "\n",
    "# Load the data using the rephrased function\n",
    "adj_matrix1 = parse_connectivity_data(\"stanweb.dat\")\n",
    "\n",
    "# Run the PageRank algorithm using Google's method\n",
    "page_ranks1, iterations1, converged_nodes1 = compute_pagerank(adj_matrix1)\n",
    "\n",
    "# Calculate and print the PageRank of the new page X\n",
    "page_rank_X = np.asarray(page_ranks1[adj_matrix1.shape[0] - 1]).ravel()[0]\n",
    "print(f\"Pagerank of page X: {page_rank_X}\")\n",
    "\n",
    "# Calculate the change in PageRank of the other pages due to the addition of the new page\n",
    "norm_difference = np.linalg.norm(\n",
    "    np.asarray(page_ranks[:adj_matrix.shape[0]]).ravel() - np.asarray(page_ranks1[:adj_matrix1.shape[0] - 1]).ravel()\n",
    ")\n",
    "print(f\"Change in Pagerank of the other pages: {norm_difference}\")\n",
    "\n",
    "# Convert PageRank vectors to flat arrays\n",
    "flattened_pr_vector = np.asarray(page_ranks).ravel()\n",
    "flattened_pr_vector1 = np.asarray(page_ranks1[:adj_matrix1.shape[0] - 1]).ravel()\n",
    "\n",
    "# Sort the PageRank vectors and get the indices in descending order\n",
    "sorted_indices_pr = flattened_pr_vector.argsort()[-len(flattened_pr_vector):][::-1]\n",
    "sorted_indices_pr1 = flattened_pr_vector1.argsort()[-len(flattened_pr_vector1):][::-1]\n",
    "\n",
    "# Print the number of differing ranks between the two sets of PageRank values\n",
    "rank_changes = np.sum(sorted_indices_pr != sorted_indices_pr1)\n",
    "print(f\"Changes in: {rank_changes} ranks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d76562-9539-464e-87e7-a996b5dc468b",
   "metadata": {},
   "source": [
    "#### 2b) Page X with in-links from Page Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b57994e4-39f3-4e76-a02f-935b866fe5b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagerank of page X: 9.867259009702931e-07\n",
      "Pagerank of page Y: 5.333653518615822e-07\n",
      "Change in Pagerank of the other pages: 3.4337013674636576e-08\n",
      "Changes in: 51477 ranks.\n"
     ]
    }
   ],
   "source": [
    "def parse_data_with_additional_edges(file_path):\n",
    "\n",
    "    # Lists to store the origin and destination pages\n",
    "    from_pages = []\n",
    "    to_pages = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read and parse each line of the data file\n",
    "        for line in file.readlines():\n",
    "            # Adjust indices to be zero-based\n",
    "            from_page = int(line.split()[0]) - 1\n",
    "            to_page = int(line.split()[1]) - 1\n",
    "\n",
    "            # Append pages to respective lists\n",
    "            from_pages.append(from_page)\n",
    "            to_pages.append(to_page)\n",
    "            \n",
    "    # Calculate number of edges and nodes\n",
    "    edges = len(from_pages)\n",
    "    nodes = len(set(from_pages) | set(to_pages))\n",
    "    \n",
    "    # Add a new edge Y -> X\n",
    "    from_pages.append(nodes)\n",
    "    to_pages.append(nodes + 1)\n",
    "    \n",
    "    # Increase the number of nodes by two for X and Y\n",
    "    nodes += 2\n",
    "    \n",
    "    # Increase the number of edges by one\n",
    "    edges += 1\n",
    "    \n",
    "    # Create the sparse adjacency matrix\n",
    "    adj_matrix2 = sparse.csr_matrix(\n",
    "        ([1] * edges, (to_pages, from_pages)),\n",
    "        shape=(nodes, nodes)\n",
    "    )\n",
    "\n",
    "    return adj_matrix2\n",
    "\n",
    "# Load the data using the rephrased function\n",
    "adj_matrix2 = parse_data_with_additional_edges(\"stanweb.dat\")\n",
    "\n",
    "# Run the PageRank algorithm using Google's method\n",
    "page_ranks2, iterations2, converged_nodes2 = compute_pagerank(adj_matrix2)\n",
    "\n",
    "# Calculate and print the PageRank of the new pages X and Y\n",
    "page_rank_Xb = np.asarray(page_ranks2[adj_matrix2.shape[0] - 1]).ravel()[0]\n",
    "print(f\"Pagerank of page X: {page_rank_Xb}\")\n",
    "page_rank_Yb = np.asarray(page_ranks2[adj_matrix2.shape[0] - 2]).ravel()[0]\n",
    "print(f\"Pagerank of page Y: {page_rank_Yb}\")\n",
    "\n",
    "# Calculate the change in PageRank of the other pages due to the addition of the new pages\n",
    "norm_difference = np.linalg.norm(\n",
    "    np.asarray(page_ranks[:adj_matrix.shape[0]]).ravel() - np.asarray(page_ranks2[:adj_matrix2.shape[0] - 2]).ravel()\n",
    ")\n",
    "print(f\"Change in Pagerank of the other pages: {norm_difference}\")\n",
    "\n",
    "# Convert PageRank vectors to flat arrays\n",
    "flattened_page_ranks = np.asarray(page_ranks).ravel()\n",
    "flattened_page_ranks2 = np.asarray(page_ranks2[:adj_matrix2.shape[0] - 2]).ravel()\n",
    "\n",
    "# Sort the PageRank vectors and get the indices in descending order\n",
    "sorted_indices_page_ranks = flattened_page_ranks.argsort()[-len(flattened_page_ranks):][::-1]\n",
    "sorted_indices_page_ranks2 = flattened_page_ranks2.argsort()[-len(flattened_page_ranks2):][::-1]\n",
    "\n",
    "# Print the number of differing ranks between the two sets of PageRank values\n",
    "rank_changes = np.sum(sorted_indices_page_ranks != sorted_indices_page_ranks2)\n",
    "print(f\"Changes in: {rank_changes} ranks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7a5306-8ca1-468a-b911-a06379b3ad2c",
   "metadata": {},
   "source": [
    "#### 2c) Page X with in-links from Page Y and Page Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9973c512-b7e6-4b71-aef0-f4bfd73af359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pagerank of page X: 1.4400850291097994e-06\n",
      "Pagerank of page Y: 5.333648255766965e-07\n",
      "Pagerank of page Z: 5.333648255766965e-07\n",
      "Change in Pagerank of the other pages: 5.662589740950828e-08\n",
      "Changes in: 52911 ranks.\n"
     ]
    }
   ],
   "source": [
    "def parse_data_with_multiple_additional_edges(file_path):\n",
    "\n",
    "    # Lists to store the origin and destination pages\n",
    "    from_pages = []\n",
    "    to_pages = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        # Read and parse each line of the data file\n",
    "        for line in file.readlines():\n",
    "            # Adjust indices to be zero-based\n",
    "            from_page = int(line.split()[0]) - 1\n",
    "            to_page = int(line.split()[1]) - 1\n",
    "\n",
    "            # Append pages to respective lists\n",
    "            from_pages.append(from_page)\n",
    "            to_pages.append(to_page)\n",
    "            \n",
    "    # Calculate number of edges and nodes\n",
    "    edges = len(from_pages)\n",
    "    nodes = len(set(from_pages) | set(to_pages))\n",
    "    \n",
    "    # Add a new edge Y -> X\n",
    "    from_pages.append(nodes)\n",
    "    to_pages.append(nodes + 2)\n",
    "    \n",
    "    # Add a new edge Z -> X\n",
    "    from_pages.append(nodes + 1)\n",
    "    to_pages.append(nodes + 2)\n",
    "\n",
    "    # Increase the number of nodes and edges\n",
    "    nodes += 3\n",
    "    edges += 2\n",
    "    \n",
    "    # Create the sparse adjacency matrix\n",
    "    adj_matrix3 = sparse.csr_matrix(\n",
    "        ([1] * edges, (to_pages, from_pages)),\n",
    "        shape=(nodes, nodes)\n",
    "    )\n",
    "\n",
    "    return adj_matrix3\n",
    "\n",
    "# Load the data using the rephrased function\n",
    "adj_matrix3 = parse_data_with_multiple_additional_edges(\"stanweb.dat\")\n",
    "\n",
    "# Run the PageRank algorithm using Google's method\n",
    "page_ranks3, iterations3, converged_nodes3 = compute_pagerank(adj_matrix3)\n",
    "\n",
    "# Calculate and print the PageRank of the new pages X, Y, and Z\n",
    "page_rank_Xc = np.asarray(page_ranks3[adj_matrix3.shape[0] - 1]).ravel()[0]\n",
    "print(f\"Pagerank of page X: {page_rank_Xc}\")\n",
    "page_rank_Yc = np.asarray(page_ranks3[adj_matrix3.shape[0] - 2]).ravel()[0]\n",
    "print(f\"Pagerank of page Y: {page_rank_Yc}\")\n",
    "page_rank_Zc = np.asarray(page_ranks3[adj_matrix3.shape[0] - 3]).ravel()[0]\n",
    "print(f\"Pagerank of page Z: {page_rank_Zc}\")\n",
    "\n",
    "# Calculate the change in PageRank of the other pages due to the addition of the new pages\n",
    "norm_difference = np.linalg.norm(\n",
    "    np.asarray(page_ranks[:adj_matrix.shape[0]]).ravel() - np.asarray(page_ranks3[:adj_matrix3.shape[0] - 3]).ravel()\n",
    ")\n",
    "print(f\"Change in Pagerank of the other pages: {norm_difference}\")\n",
    "\n",
    "# Convert PageRank vectors to flat arrays\n",
    "flattened_page_ranks = np.asarray(page_ranks).ravel()\n",
    "flattened_page_ranks3 = np.asarray(page_ranks3[:adj_matrix3.shape[0] - 3]).ravel()\n",
    "\n",
    "# Sort the PageRank vectors and get the indices in descending order\n",
    "sorted_indices_page_ranks = flattened_page_ranks.argsort()[-len(flattened_page_ranks):][::-1]\n",
    "sorted_indices_page_ranks3 = flattened_page_ranks3.argsort()[-len(flattened_page_ranks3):][::-1]\n",
    "\n",
    "# Print the number of differing ranks between the two sets of PageRank values\n",
    "rank_changes = np.sum(sorted_indices_page_ranks != sorted_indices_page_ranks3)\n",
    "print(f\"Changes in: {rank_changes} ranks.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9596d05-5789-471a-b714-bbb91946a82a",
   "metadata": {},
   "source": [
    "#### 2d) Page X with out-links to popular pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5c4d88e8-fc03-44bd-a2f2-00947e146b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding links from X to older, popular pages. Pagerank of page X: 1.4400732985803644e-06\n",
      "Adding links from Y to older, popular pages. Pagerank of page X: 9.912111253145325e-07\n"
     ]
    }
   ],
   "source": [
    "# Define a function to load and parse the initial data\n",
    "def load_initial_data(data):\n",
    "    from_pages = []\n",
    "    to_pages = []\n",
    "    dict_in = {}\n",
    "\n",
    "    with open(data, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            from_page = int(line.split()[0]) - 1\n",
    "            to_page = int(line.split()[1]) - 1\n",
    "            from_pages.append(from_page)\n",
    "            to_pages.append(to_page)\n",
    "            if to_page not in dict_in:\n",
    "                dict_in[to_page] = [from_page]\n",
    "            else:\n",
    "                dict_in[to_page].append(from_page)\n",
    "                \n",
    "    edges = len(from_pages)\n",
    "    nodes = len(set(from_pages) | set(to_pages))\n",
    "    \n",
    "    csr_matrix = sparse.csr_matrix(([1] * edges, (to_pages, from_pages)), shape=(nodes, nodes))\n",
    "    \n",
    "    return csr_matrix, dict_in\n",
    "\n",
    "# Load the initial data\n",
    "adj_matrix, dict_in = load_initial_data(\"stanweb.dat\")\n",
    "\n",
    "# Run the initial PageRank calculation\n",
    "page_ranks, iterations, converged_nodes = compute_pagerank(adj_matrix)\n",
    "\n",
    "# Determine top 100 - most popular pages based on PageRank\n",
    "indices_page_ranks = np.asarray(page_ranks).ravel().argsort()[-len(page_ranks):][::-1]\n",
    "top_100_pagerank = indices_page_ranks[:100]\n",
    "\n",
    "# Determine top 100 - most popular pages based on in-degree\n",
    "list_degree = np.asarray([len(dict_in[x]) for x in sorted(list(dict_in.keys()))])\n",
    "indices_degree = list_degree.argsort()[-len(list_degree):][::-1]\n",
    "top_100_indegree = indices_degree[:100]\n",
    "\n",
    "def parse_data_with_links_to_top_pages(file_path, top_100):\n",
    "\n",
    "    from_pages = []\n",
    "    to_pages = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            from_page = int(line.split()[0]) - 1\n",
    "            to_page = int(line.split()[1]) - 1\n",
    "            from_pages.append(from_page)\n",
    "            to_pages.append(to_page)\n",
    "            \n",
    "    edges = len(from_pages)\n",
    "    nodes = len(set(from_pages) | set(to_pages))\n",
    "    \n",
    "    from_pages.append(nodes)\n",
    "    to_pages.append(nodes + 2)\n",
    "    \n",
    "    from_pages.append(nodes + 1)\n",
    "    to_pages.append(nodes + 2)\n",
    "    \n",
    "    for node in top_100:\n",
    "        from_pages.append(nodes + 2)\n",
    "        to_pages.append(node)\n",
    "    \n",
    "    nodes += 3\n",
    "    edges += 102\n",
    "    \n",
    "    adj_matrix4 = sparse.csr_matrix(([1] * edges, (to_pages, from_pages)), shape=(nodes, nodes))\n",
    "\n",
    "    return adj_matrix4\n",
    "\n",
    "def parse_data_with_links_from_y_to_top_pages(file_path, top_100):\n",
    " \n",
    "    from_pages = []\n",
    "    to_pages = []\n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file.readlines():\n",
    "            from_page = int(line.split()[0]) - 1\n",
    "            to_page = int(line.split()[1]) - 1\n",
    "            from_pages.append(from_page)\n",
    "            to_pages.append(to_page)\n",
    "            \n",
    "    edges = len(from_pages)\n",
    "    nodes = len(set(from_pages) | set(to_pages))\n",
    "    \n",
    "    from_pages.append(nodes)\n",
    "    to_pages.append(nodes + 2)\n",
    "    \n",
    "    from_pages.append(nodes + 1)\n",
    "    to_pages.append(nodes + 2)\n",
    "    \n",
    "    for node in top_100:\n",
    "        from_pages.append(nodes + 1)\n",
    "        to_pages.append(node)\n",
    "\n",
    "    nodes += 3\n",
    "    edges += 102\n",
    "    \n",
    "    adj_matrix5 = sparse.csr_matrix(([1] * edges, (to_pages, from_pages)), shape=(nodes, nodes))\n",
    "\n",
    "    return adj_matrix5\n",
    "\n",
    "# Load the data using the rephrased functions\n",
    "adj_matrix4 = parse_data_with_links_to_top_pages(\"stanweb.dat\", top_100_pagerank)\n",
    "page_ranks4, iterations4, converged_nodes4 = compute_pagerank(adj_matrix4)\n",
    "\n",
    "adj_matrix5 = parse_data_with_links_from_y_to_top_pages(\"stanweb.dat\", top_100_pagerank)\n",
    "page_ranks5, iterations5, converged_nodes5 = compute_pagerank(adj_matrix5)\n",
    "\n",
    "# If we add links from X to older, popular pages\n",
    "page_rank_Xd = np.asarray(page_ranks4[adj_matrix4.shape[0] - 1]).ravel()[0]\n",
    "print(f\"Adding links from X to older, popular pages. Pagerank of page X: {page_rank_Xd}\")\n",
    "\n",
    "# If we add links from Y to older, popular pages\n",
    "page_rank_Xd2 = np.asarray(page_ranks5[adj_matrix5.shape[0] - 1]).ravel()[0]\n",
    "print(f\"Adding links from Y to older, popular pages. Pagerank of page X: {page_rank_Xd2}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
